{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113002,"databundleVersionId":13471427,"sourceType":"competition"},{"sourceId":600891,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":450241,"modelId":466601}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ResNet contineud\n- Load prev trained model, more epochs, decrease learning rate","metadata":{}},{"cell_type":"code","source":"!pip install iterative-stratification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.image as mpimg\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import Sequence\nimport cv2\nimport os\nimport sys\nfrom io import StringIO\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import Sequence\nimport tensorflow.keras.applications.resnet50 as resnet\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.listdir(\"/kaggle/input\"))\n\n# Path to competition dataset\ndata_dir = \"/kaggle/input/grand-xray-slam-division-b\"\n# Check what files are inside\nprint('Filenames of the data', os.listdir(data_dir))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the training CSV metadata with labels\ntrain = pd.read_csv(\"/kaggle/input/grand-xray-slam-division-b/train2.csv\")\n\nprint('Metadata shape:',train.shape)\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Feature & Target Preperation\n# Define labels\nconditions = [\n    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n    'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n    'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n]\n# Features you want\nfeatures = [\"ViewCategory\", \"ViewPosition\", \"Age\", \"Sex\"]\n\n# Encode categorical features\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_enc = train.copy()   # train data encoded\nfor col in [\"ViewCategory\", \"ViewPosition\", \"Sex\"]:  # features that can be encoded\n    le = LabelEncoder()\n    train_enc[col] = le.fit_transform(train[col].astype(str))\n\nX = train_enc[features].values\ny = train[conditions].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.shape) # 4 features (ViewCategory, ViewPosition, Age, Sex)\nprint(y.shape)  # 14 conditions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Adding ViewBalancing for Stratification: ViewCategory= Frontal, Lateral; since ViewCategory is unbalanced\n\n# One-hot encode ViewCategory and append to \nview_onehot = pd.get_dummies(train[\"ViewCategory\"], prefix=\"view\").values\n\ny_aug = np.hstack([y, view_onehot])  # augmented target matrix (added ViewCategory as y to stratify and reduce bias)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MSKF","metadata":{}},{"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# 3. Multilabel Stratified K-Fold Split\nmskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfolds = []\nfor fold, (train_idx, val_idx) in enumerate(mskf.split(X, y_aug)):\n    print(f\"Fold {fold}\")\n    print(\" Train:\", len(train_idx), \" Val:\", len(val_idx))\n\n    train_df = train.iloc[train_idx].reset_index(drop=True)\n    val_df   = train.iloc[val_idx].reset_index(drop=True)\n\n    # Check condition + view balance\n    print(\"  Train views:\", train_df[\"ViewCategory\"].value_counts().to_dict())\n    print(\"  Val views:\", val_df[\"ViewCategory\"].value_counts().to_dict())\n    print(\"  Train labels sum:\", train_df[conditions].sum().to_dict())\n    print(\"  Val labels sum:\", val_df[conditions].sum().to_dict())\n    print(\"-\"*60)\n\n# Use first fold for training\ntrain_df, val_df = folds[0]\nprint(f\"✅ Selected Fold 1 — Train {train_df.shape}, Val {val_df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# data generator","metadata":{}},{"cell_type":"code","source":"class XRayDataGenerator(Sequence):\n    def __init__(self, dataframe, batch_size=32, img_size=(224, 224), is_test=False, **kwargs):\n        super().__init__(**kwargs)\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.is_test = is_test\n        self.image_dir = '/kaggle/input/grand-xray-slam-division-b/train2/' if not is_test else '/kaggle/input/grand-xray-slam-division-b/test2/'\n        self.conditions = conditions\n        \n        if not os.path.exists(self.image_dir):\n            print(f\"Error: Directory {self.image_dir} not found.\")\n            raise FileNotFoundError(f\"Directory {self.image_dir} missing.\")\n    \n    def __len__(self):\n        return (len(self.dataframe) + self.batch_size - 1) // self.batch_size\n    \n    def __getitem__(self, idx):\n        start = idx * self.batch_size\n        end = min(start + self.batch_size, len(self.dataframe))\n        batch_data = self.dataframe.iloc[start:end]\n        \n        images, labels = [], []\n        \n        for _, row in batch_data.iterrows():\n            img_path = os.path.join(self.image_dir, row['Image_name'])\n            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n            \n            if img is not None and img.shape[0] > 0 and img.shape[1] > 0:\n                img = cv2.resize(img, self.img_size)\n                img = resnet.preprocess_input(img)\n                images.append(img)\n                \n                if not self.is_test:\n                    labels.append(row[self.conditions].values.astype(np.float32))\n        \n        if not images:\n            dummy_img = np.zeros((*self.img_size, 3), dtype=np.float32)\n            images.append(dummy_img)\n            if not self.is_test:\n                labels.append(np.zeros(len(self.conditions), dtype=np.float32))\n        \n        if not self.is_test:\n            return np.array(images), np.array(labels)\n        else:\n            return np.array(images)\n\n# Create generators\nbatch_size = 32\ntrain_generator = XRayDataGenerator(train_df, batch_size=batch_size)\nval_generator = XRayDataGenerator(val_df, batch_size=batch_size)\nprint(\"Data generators created.\")\n\n# Also define test_df (important for submission)\ntest_df = pd.read_csv(\"/kaggle/input/grand-xray-slam-division-b/sample_submission_2.csv\")\ntest_df[\"Image_name\"] = test_df[\"Image_name\"].astype(str)\n\nprint(\"✅ Data generators ready — Train, Val, and Test loaded.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# resnet improved ","metadata":{}},{"cell_type":"code","source":"# from tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import ResNet50\n\ndef build_resnet_model(num_classes=14):\n    # Load Resnet with cached ImageNet weights\n    base_model = ResNet50(weights=\"imagenet\",  include_top=False, input_shape=(224, 224, 3))\n    \n    print(\"✅ Weights loaded successfully.\")\n    base_model.trainable = False   # freeze backbone for now\n    \n    # add custom head\n    inputs = base_model.input\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256, activation=\"relu\")(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(num_classes, activation=\"sigmoid\")(x)  # multilabel\n    \n    model = Model(inputs, outputs)\n    return model\n    \n# model = build_resnet_model()\nfrom tensorflow.keras.models import load_model\n\nmodel = load_model(\"/kaggle/input/newresnet/final_resnet_model.h5\", compile=False)\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0001), loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.AUC(name=\"auc\")]\n)\n\nprint(\"Model Architecture: ResNet50 + Custom Head\")\nprint(f\"Total parameters: {model.count_params():,}\")\ntrainable_params = sum([tf.size(v).numpy() for v in model.trainable_variables])\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Non-trainable parameters: {model.count_params() - trainable_params:,}\")\nprint(\"Model compiled successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\ncallbacks = [\n    ModelCheckpoint(\"resnet_finetune.weights.h5\", save_weights_only=True, save_best_only=True,\n                    monitor=\"val_auc\", mode=\"max\", verbose=1),\n    ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=2, mode=\"max\", verbose=1),\n    EarlyStopping(monitor=\"val_auc\", patience=5, mode=\"max\", restore_best_weights=True)\n]\nhistory = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=5,\n    verbose=1,\n    callbacks=callbacks\n)\nval_auc = history.history.get(\"val_auc\", [0])[-1]\nprint(f\"✅ Continued fine-tuning done — new Validation AUC ≈ {val_auc:.4f}\")\nmodel.save(\"final_resnet_finetuned.h5\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}